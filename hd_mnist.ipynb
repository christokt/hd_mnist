{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11070ab70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Set torch seed\n",
    "torch.manual_seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Hyperparameters\n",
    "HD_DIMENSION = 20000\n",
    "NUM_CLASSES = 10\n",
    "THRESHOLD = 0.0\n",
    "BATCH_SIZE = 512\n",
    "IMG_LEN = 28 * 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = torch.device('mps')\n",
    "print('Using device:', device)\n",
    "\n",
    "# Load MNIST Data\n",
    "transform = transforms.ToTensor()\n",
    "train_data = torchvision.datasets.MNIST(root='data', train=True, transform=transform, download=True)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(root='data', train=False, transform=transform, download=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate HD vectors\n",
    "def generate_hd_vectors(images, proj):\n",
    "\n",
    "    # Adjust proj to sit between -1 and 1\n",
    "    proj = (proj * 2) - 1\n",
    "\n",
    "    # Send images to device\n",
    "    images = images.to(device)\n",
    "\n",
    "    # Generate hypervectors and create binary vector\n",
    "    hd_vectors = torch.sign(images @ proj.T)\n",
    "    \n",
    "    return hd_vectors.to(device)\n",
    "\n",
    "# Classify hd vectors using cosine similarity\n",
    "def classify_hd_vectors(hd_vectors, hd_memory):\n",
    "    distances = torch.cdist(hd_vectors, hd_memory, p=2)\n",
    "    \n",
    "    # Find the index of the minimum distance\n",
    "    min_distances, min_indices = torch.min(distances, dim=1)\n",
    "    return min_indices.to(device)\n",
    "\n",
    "def create_hd_memory(hd_vectors, labels, HD_DIMENSION):\n",
    "    # Initialize HD Memory for classes\n",
    "    hd_memory = torch.zeros((NUM_CLASSES, HD_DIMENSION), device=device)\n",
    "\n",
    "    # Create HD Memory\n",
    "    for i, vec in enumerate(hd_vectors):\n",
    "        hd_memory[train_data.targets[i]] += vec\n",
    "\n",
    "    return torch.sign(hd_memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create projection matrix\n",
    "proj = torch.rand((HD_DIMENSION, IMG_LEN), device=device, dtype=torch.float32)\n",
    "\n",
    "# Create hyper vectors\n",
    "hd_vectors = generate_hd_vectors(train_data.data.view(train_data.data.shape[0], -1) / 255, proj)\n",
    "\n",
    "hd_memory = create_hd_memory(hd_vectors, train_data.targets, HD_DIMENSION)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to binary for faster computation\n",
    "def to_binary(hd_vectors, hd_memory):\n",
    "    hd_vectors = (hd_vectors == 1).to(torch.float32)\n",
    "    hd_memory = (hd_memory == 1).to(torch.float32)\n",
    "    return hd_vectors, hd_memory\n",
    "\n",
    "hd_vectors, hd_memory = to_binary(hd_vectors, hd_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data: 81.26333618164062%\n"
     ]
    }
   ],
   "source": [
    "# Testing training data\n",
    "classifications = classify_hd_vectors(hd_vectors, hd_memory)\n",
    "\n",
    "correct = torch.sum(classifications == train_data.targets.to(device))\n",
    "total = len(train_data.targets)\n",
    "print(f\"Accuracy on training data: {correct / total * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 77.60000000000001%\n"
     ]
    }
   ],
   "source": [
    "# Testing on test data\n",
    "hd_vectors_test = generate_hd_vectors(test_data.data.view(test_data.data.shape[0], -1) / 255, proj)\n",
    "hd_vectors_test = (hd_vectors_test == 1).to(torch.float32)\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "classifications = classify_hd_vectors(hd_vectors_test, hd_memory)\n",
    "\n",
    "for i, classification in enumerate(classifications):\n",
    "    if classification == test_data.targets[i]:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "\n",
    "print(f\"Accuracy on test data: {correct / total * 100}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare accuracy when applying random noise to images\n",
    "noise_level = 0.85\n",
    "\n",
    "noise_matrix = torch.randn((test_data.data.shape[0], IMG_LEN), device=device) * noise_level\n",
    "\n",
    "# Plot noise matrix\n",
    "noisy_test_data = (test_data.data.view(test_data.data.shape[0], -1) / 255).to(device) + noise_matrix\n",
    "\n",
    "plt.imshow(noisy_test_data[0].view(28, 28).cpu(), cmap='gray')\n",
    "\n",
    "hd_vectors_noise = generate_hd_vectors(noisy_test_data, proj)\n",
    "hd_vectors_noise = (hd_vectors_noise == 1).to(torch.float32)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data with noise: 49.23999786376953%\n"
     ]
    }
   ],
   "source": [
    "# Test noisy test data\n",
    "classifications = classify_hd_vectors(hd_vectors_noise, hd_memory)\n",
    "\n",
    "correct = torch.sum(classifications == test_data.targets.to(device))\n",
    "total = len(test_data.targets)\n",
    "print(f\"Accuracy on test data with noise: {correct / total * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining and training an MLP for MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.4091\n",
      "Accuracy: 93.20%\n",
      "Epoch [2/10], Loss: 0.1655\n",
      "Accuracy: 95.70%\n",
      "Epoch [3/10], Loss: 0.0896\n",
      "Accuracy: 97.01%\n",
      "Epoch [4/10], Loss: 0.0557\n",
      "Accuracy: 97.79%\n",
      "Epoch [5/10], Loss: 0.0170\n",
      "Accuracy: 98.11%\n",
      "Epoch [6/10], Loss: 0.0752\n",
      "Accuracy: 98.55%\n",
      "Epoch [7/10], Loss: 0.1662\n",
      "Accuracy: 98.89%\n",
      "Epoch [8/10], Loss: 0.0333\n",
      "Accuracy: 99.11%\n",
      "Epoch [9/10], Loss: 0.0204\n",
      "Accuracy: 99.14%\n",
      "Epoch [10/10], Loss: 0.0428\n",
      "Accuracy: 99.24%\n"
     ]
    }
   ],
   "source": [
    "# Create simple neural network\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(IMG_LEN, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(256, NUM_CLASSES)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, IMG_LEN)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "# Create neural network\n",
    "model = NeuralNetwork().to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train neural network\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Reshape images to (BATCH_SIZE, 28*28)\n",
    "        images = images.view(-1, 28 * 28).to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Print the loss and accuracy after each epoch\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Accuracy\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.view(-1, 28 * 28).to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.08%\n"
     ]
    }
   ],
   "source": [
    "# Testing loop to evaluate the model using test_loader\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, 28 * 28).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    test_accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {test_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define and train a Vision Transformer for MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0725\n",
      "Accuracy: 95.74%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jvelasquez/ml_projects/hd_compute/hd_mnist/hd_mnist.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jvelasquez/ml_projects/hd_compute/hd_mnist/hd_mnist.ipynb#X21sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m     outputs \u001b[39m=\u001b[39m v(images)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jvelasquez/ml_projects/hd_compute/hd_mnist/hd_mnist.ipynb#X21sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jvelasquez/ml_projects/hd_compute/hd_mnist/hd_mnist.ipynb#X21sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jvelasquez/ml_projects/hd_compute/hd_mnist/hd_mnist.ipynb#X21sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jvelasquez/ml_projects/hd_compute/hd_mnist/hd_mnist.ipynb#X21sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch [\u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m \u001b[39m\u001b[39m+\u001b[39m\u001b[39m \u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mnum_epochs\u001b[39m}\u001b[39;00m\u001b[39m], Loss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/conda_envs/hd_compute/lib/python3.10/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/conda_envs/hd_compute/lib/python3.10/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from vit_pytorch import ViT\n",
    "\n",
    "v = ViT(\n",
    "    image_size = 28,\n",
    "    patch_size = 7,\n",
    "    num_classes = 10,\n",
    "    dim = 256,\n",
    "    depth = 6,\n",
    "    heads = 16,\n",
    "    mlp_dim = 512,\n",
    "    dropout = 0.1,\n",
    "    emb_dropout = 0.1,\n",
    "    channels = 1\n",
    ").to(device)\n",
    "\n",
    "# Train the model\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(v.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = v(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Accuracy\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = v(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'Accuracy: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test high dimensional vector MNIST classification with varying dimensions and noise levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test noisy test data with varying noise levels\n",
    "noise_levels = np.arange(0, 1, 0.02)\n",
    "\n",
    "# Make varying levels of noise from 10000 upto HD_DIMENSION\n",
    "noise_levels = np.arange(10000, HD_DIMENSION, 10000)\n",
    "\n",
    "total = len(test_data.targets)\n",
    "hd_history = {}\n",
    "\n",
    "# Initialize label-wise counters\n",
    "label_counter = {i: 0 for i in range(10)}\n",
    "label_total = {i: 0 for i in range(10)}\n",
    "\n",
    "for hd_dim in hd_dims:\n",
    "    # Create projection matrix\n",
    "    proj = torch.rand((hd_dim, IMG_LEN), device=device, dtype=torch.float32)\n",
    "\n",
    "    # Create hyper vectors\n",
    "    hd_vectors = generate_hd_vectors(train_data.data.view(train_data.data.shape[0], -1) / 255, proj)\n",
    "\n",
    "    # Create HD Memory\n",
    "    hd_memory = create_hd_memory(hd_vectors, train_data.targets, hd_dim)\n",
    "\n",
    "    # Convert to binary for faster computation\n",
    "    hd_vectors, hd_memory = to_binary(hd_vectors, hd_memory)\n",
    "\n",
    "    accuracies_hd = []\n",
    "    for noise_level in noise_levels:\n",
    "        noise_matrix = torch.randn((test_data.data.shape[0], IMG_LEN), device=device) * noise_level\n",
    "\n",
    "        # Plot noise matrix\n",
    "        noisy_test_data = (test_data.data.view(test_data.data.shape[0], -1) / 255).to(device) + noise_matrix\n",
    "\n",
    "        hd_vectors_noise = generate_hd_vectors(noisy_test_data, proj)\n",
    "        hd_vectors_noise = (hd_vectors_noise == 1).to(torch.float32)\n",
    "\n",
    "        classifications = classify_hd_vectors(hd_vectors_noise, hd_memory)\n",
    "\n",
    "        correct = torch.sum(classifications.cpu() == test_data.targets)\n",
    "\n",
    "        # Update label-wise counters\n",
    "        for true, pred in zip(test_data.targets, classifications.cpu()):\n",
    "            label_total[true.item()] += 1\n",
    "            if true == pred:\n",
    "                label_counter[true.item()] += 1\n",
    "                \n",
    "        accuracies_hd.append(correct / total * 100)\n",
    "\n",
    "    hd_history[hd_dim] = accuracies_hd\n",
    "\n",
    "# Calculate and print label-wise accuracies\n",
    "label_accuracies = {label: (label_counter[label] / label_total[label]) * 100 for label in label_counter}\n",
    "print(\"Label-wise accuracies:\", label_accuracies)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test MLP with varying noise levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test neural network with progressively more noise\n",
    "accuracies_net = []\n",
    "\n",
    "for noise_level in noise_levels:\n",
    "\n",
    "    # Test noisy test data\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_loader):\n",
    "            images = images.view(-1, 28 * 28).to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Add noise to images\n",
    "            noise_matrix = torch.randn((images.shape[0], IMG_LEN), device=device) * noise_level\n",
    "            images = images + noise_matrix\n",
    "\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracies_net.append(correct / total * 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test ViT with varying levels of noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_vit = []\n",
    "\n",
    "for noise_level in noise_levels:\n",
    "    \n",
    "        # Test noisy test data\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for i, (images, labels) in enumerate(test_loader):\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "    \n",
    "                # Add noise to images\n",
    "                noise_matrix = torch.randn((images.shape[0], 28, 28), device=device) * noise_level\n",
    "                images = images + noise_matrix\n",
    "    \n",
    "                outputs = v(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        acc_vit.append(correct / total * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display MNIST images with noise to get an idea of how noisy the images are to the human eye"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a random image from the test set with the various noise levels\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "index = torch.randint(0, len(test_data.targets), size=(1,)).item()\n",
    "for i, noise_level in enumerate(np.arange(0, 1, 0.1)):\n",
    "    noise_matrix = torch.randn((test_data.data.shape[0], IMG_LEN)) * noise_level\n",
    "\n",
    "    # Create noisy image\n",
    "    noisy_test_data = (test_data.data.view(test_data.data.shape[0], -1) / 255) + noise_matrix\n",
    "\n",
    "    # Create subplot and display the image\n",
    "    plt.subplot(3, 4, i + 1)  # Assuming you have 10 images (0, 0.1, ..., 0.9), adjust the dimensions as necessary\n",
    "    plt.imshow(noisy_test_data[index].view(28, 28), cmap='gray')\n",
    "    plt.title(f'Noise Level: {noise_level:.1f}')\n",
    "    plt.axis('off')  # Turn off axis numbers and ticks\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for hd_dim in hd_history:\n",
    "    plt.plot(noise_levels, hd_history[hd_dim])\n",
    "plt.plot(noise_levels, accuracies_net)\n",
    "plt.plot(noise_levels, acc_vit)\n",
    "plt.legend([\"HD Dimension: 10000\", \"HD Dimension: 20000\", \"HD Dimension: 30000\", \"HD Dimension: 40000\", \"HD Dimension: 50000\", \"HD Dimension: 60000\", \"HD Dimension: 70000\", \"Neural Network\"])\n",
    "plt.xlabel(\"Noise Level\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Noise Level for Neural Network\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = list(label_accuracies.keys())\n",
    "# accuracies = list(label_accuracies.values())\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(labels, accuracies, color='blue')\n",
    "# plt.xlabel('Labels')\n",
    "# plt.ylabel('Accuracy (%)')\n",
    "# plt.title('Label-wise Accuracy')\n",
    "# plt.xticks(labels)\n",
    "# plt.ylim([0, 100])  # Assuming accuracies are in percentages\n",
    "# plt.grid(True, axis='y')\n",
    "\n",
    "# # Annotate the bars with the actual percentages\n",
    "# for i, v in enumerate(accuracies):\n",
    "#     plt.text(i, v + 1, f\"{v:.2f}\", ha='center', va='bottom')\n",
    "\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
